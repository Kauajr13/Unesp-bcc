{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kauajr13/Unesp-bcc/blob/main/IA/trabalho_agentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nt_Y8mWLoSr2"
      },
      "id": "Nt_Y8mWLoSr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "76863271",
      "metadata": {
        "id": "76863271"
      },
      "source": [
        "\n",
        "# Trabalho: **Meu Primeiro Agente** — Aspirador (2 salas)\n",
        "\n",
        "**Aluno(a):** Kauã Junior Silva Soares\n",
        "\n",
        "**Disciplina:** Inteligência Artificial  \n",
        "\n",
        "**Data:** 15/09/2025\n",
        "\n",
        "> **Importante:** Este notebook contém **esqueletos** com parte da solução já implementada.\n",
        "> Você deve implementar as demais funções pedidas, rodar experimentos e responder às perguntas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f8aacec",
      "metadata": {
        "id": "2f8aacec"
      },
      "source": [
        "\n",
        "## Objetivos\n",
        "1. Definir o problema via **PEAS**.  \n",
        "2. Especificar formalmente **S, A, T, h, r, P** e explicar cada símbolo.  \n",
        "3. Implementar um **agente reflexo** e medir a **performance** passo a passo.  \n",
        "4. Discutir **limitações** e propor **uma melhoria simples**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af49fab6",
      "metadata": {
        "id": "af49fab6"
      },
      "source": [
        "\n",
        "## Instruções\n",
        "- Preencha todo conteúdo solicitado (texto e código).  \n",
        "- Use a notação apresentada na aula: `s=(p,d_L,d_R)`, `o=h(s)`, `a∈A`, `r(s,a,s')`, `P`.  \n",
        "- Quando terminar, exporte em **PDF** e entregue **PDF + .ipynb** no _classroom_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86626733",
      "metadata": {
        "id": "86626733"
      },
      "source": [
        "\n",
        "---\n",
        "## 1) PEAS — Definição do problema\n",
        "Explique concisamente e escreva as fórmulas pedidas.\n",
        "\n",
        "**P — Performance:**  \n",
        "- Defina `r(s,a,s')` e $P = \\sum_{t=0}^{H-1} r_t$\n",
        "  - `r(s,a,s')` é a recompensa recebida após a ação `a` levar do estado `s` ao estado `s′`\n",
        "  - `$P = \\sum_{t=0}^{H-1} r_t$` seria o desempenho total ao longo de H passos, sendo dessa forma a soma de todas as recompensas instantâneas\n",
        "\n",
        "- Declare os parâmetros (λ e custos).\n",
        "  - λ>0: ganho (valor) por remoção de uma unidade de sujeira\n",
        "  - os custos seriam os custos por ação realizada, no aspirador seria o custo de ir para esquerda,direita ou aspirar\n",
        "\n",
        "**E — Environment:**  \n",
        "- Descreva as duas salas, estados e por que |S| = 8.\n",
        "  - O estado s é s=(p,dL,dR) onde:\n",
        "    - p ∈ {L,R} é a posição do agente (esquerda ou direita);\n",
        "    - dL ∈ {0,1} é o bit de sujeira da sala esquerda (0=limpa, 1=suja);\n",
        "    - dR ∈ {0,1} é o bit de sujeira da sala direita (0=limpa, 1=suja);\n",
        "    - Como cada componente apresenta 2 escolhas possíveis, logo o número de estados é |S| = 2^3 = 8\n",
        "\n",
        "**A — Actuators:**  \n",
        "- Liste {ESQ, DIR, ASP} e comente custos.\n",
        "  - ESQ: move o agente para a sala da esquerda\n",
        "  - DIR: move o agente para a sala da direita\n",
        "  - ASP: aspira a sala atual, removendo a sujeira se houver\n",
        "\n",
        "  - custos:\n",
        "    - c(ESQ): custo de se mover para a esquerda\n",
        "    - c(DIR): custo de se mover para a direita\n",
        "    - c(ASP): custo de aspirar\n",
        "\n",
        "\n",
        "**S — Sensors:**  \n",
        "- Defina `h: S -> O` no modo local (`h(s)=(p, sujeira_aqui)`).\n",
        "\n",
        "  - Função de percepção: `h: S -> O` definida no modo local por `h(s)=(p, sujeira_aqui)` onde `p` é a posição do agente e `sujeira_aqui` é o bit de sujeira onde o agente está"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600e6535",
      "metadata": {
        "id": "600e6535"
      },
      "source": [
        "\n",
        "---\n",
        "## 2) Especificação formal — S, A, T, h, r, P  **(TODO)**\n",
        "Preencha a definição com sua própria redação e notação clara.\n",
        "\n",
        "- **Estados:**  `S={(p,dL​,dR​)∣p∈{L,R},dL​∈{0,1},dR​∈{0,1}}`  \n",
        "- **Ações:**    `A = {ESQ, DIR, ASP}`  \n",
        "- **Transição determinística:** `T: S x A -> S`  \n",
        "  - `T(s,ASP) = {(L,0,dR​)}, se p=L, ou {(R,dL​,0)}, se p=R`\n",
        "  - `T(s,DIR) = (R,dL​,dR​)`\n",
        "  - `T(s,ESQ) = (L,dL,dR)`\n",
        "- **Percepção:** `h: S -> O` com observação local\n",
        "    - `h(p,dL​,dR​)=(p,dp​)`\n",
        "    - `onde dp=dL se p=L e dP = dR se p = R`\n",
        "- **Recompensa r e performance P:**\n",
        "  - λ>0: valor ganho por remover uma unidade de sujeira\n",
        "  - c(ESQ), c(DIR), c(ASP) >= 0 : custos das ações\n",
        "  - H : horizonte finito do episódio\n",
        "  - Recompensa imediata: `r(s, a, s') = λ * 1{ a = ASP  and  d_p(s) = 1  and  d_p(s') = 0 }  -  c(a)`\n",
        "\n",
        "  - Performance: `P = sum_{t=0}^{H-1} r_t onde r_t = r(s_t, a_t, s_{t+1})`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0e1de87b",
      "metadata": {
        "id": "0e1de87b"
      },
      "outputs": [],
      "source": [
        "# inicio do código\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict, Any\n",
        "\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "49ee6882",
      "metadata": {
        "id": "49ee6882"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Env2Salas:\n",
        "    p: str   # 'L' ou 'R'\n",
        "    dL: int  # 0=limpo, 1=sujo\n",
        "    dR: int  # 0=limpo, 1=sujo\n",
        "\n",
        "    def copy(self):\n",
        "        return Env2Salas(self.p, self.dL, self.dR)\n",
        "\n",
        "    def h(self) -> Tuple[str, int]:\n",
        "        \"\"\"Retorna (posicao, sujeira_aqui).\"\"\"\n",
        "        sujeira_aqui = self.dL if self.p == 'L' else self.dR\n",
        "        return (self.p, sujeira_aqui)\n",
        "\n",
        "    def T(self, a: str) -> 'Env2Salas':\n",
        "        \"\"\"Transição determinística.\"\"\"\n",
        "\n",
        "        s_ = self.copy()\n",
        "\n",
        "        if a == 'ASP':\n",
        "            if s_.p == 'L':\n",
        "                s_.dL = 0\n",
        "            else:\n",
        "                s_.dR = 0\n",
        "        elif a == 'DIR':\n",
        "            s_.p = 'R'\n",
        "        elif a == 'ESQ':\n",
        "            s_.p = 'L'\n",
        "\n",
        "        return s_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0cca9655",
      "metadata": {
        "id": "0cca9655"
      },
      "outputs": [],
      "source": [
        "def r(s: Env2Salas, a: str, s_: Env2Salas, lam: float,\n",
        "      c_esq: float, c_dir: float, c_asp: float) -> float:\n",
        "    \"\"\"Recompensa imediata.\"\"\"\n",
        "    custos = {'ESQ': c_esq, 'DIR': c_dir, 'ASP': c_asp}\n",
        "    custo_a = custos.get(a, 0) # Retorna 0 para ações desconhecidas\n",
        "\n",
        "    recompensa_limpeza = 0.0\n",
        "    if a == 'ASP':\n",
        "        if s.p == 'L' and s.dL == 1 and s_.dL == 0:\n",
        "            recompensa_limpeza = lam\n",
        "        elif s.p == 'R' and s.dR == 1 and s_.dR == 0:\n",
        "            recompensa_limpeza = lam\n",
        "\n",
        "    return recompensa_limpeza - custo_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8deaff6a",
      "metadata": {
        "id": "8deaff6a"
      },
      "outputs": [],
      "source": [
        "def politica_reflexo(obs: Tuple[str, int]) -> str:\n",
        "    \"\"\"Política reflexo simples.\"\"\"\n",
        "    p, sujeira_aqui = obs\n",
        "    if sujeira_aqui == 1:\n",
        "        return 'ASP'\n",
        "    else:\n",
        "        return 'DIR' if p == 'L' else 'ESQ'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ac0af4d1",
      "metadata": {
        "id": "ac0af4d1"
      },
      "outputs": [],
      "source": [
        "def simular(s0: Env2Salas, lam=1.0, c_esq=1.0, c_dir=1.0, c_asp=1.0, H=20):\n",
        "    \"\"\"Roda a simulação por H passos ou até o ambiente estar totalmente limpo.\"\"\"\n",
        "    historico = []\n",
        "    s = s0.copy()\n",
        "\n",
        "    for t in range(H):\n",
        "        if s.dL == 0 and s.dR == 0:\n",
        "            break\n",
        "\n",
        "        obs = s.h()\n",
        "        a = politica_reflexo(obs)\n",
        "        s_ = s.T(a)\n",
        "\n",
        "        recompensa_imediata = r(s, a, s_, lam, c_esq, c_dir, c_asp)\n",
        "\n",
        "        historico.append({\n",
        "            't': t,\n",
        "            's': s.copy(),\n",
        "            'a': a,\n",
        "            's_': s_.copy(),\n",
        "            'r': recompensa_imediata\n",
        "        })\n",
        "\n",
        "        s = s_\n",
        "\n",
        "    df_historico = pd.DataFrame(historico)\n",
        "\n",
        "    recompensa_total = df_historico['r'].sum()\n",
        "\n",
        "    return historico, recompensa_total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7063209",
      "metadata": {
        "id": "d7063209"
      },
      "source": [
        "\n",
        "### Testes mínimos (rode quando terminar as implementações)\n",
        "Descomente os `assert` abaixo **depois** de implementar `h`, `T`, `r`, `politica_reflexo`, `simular`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "10226c41",
      "metadata": {
        "id": "10226c41"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # --- Descomente os asserts após implementar ---\n",
        "s = Env2Salas('L',1,1)\n",
        "assert s.h() == ('L', 1)\n",
        "s1 = s.T('ASP')\n",
        "assert (s1.p, s1.dL, s1.dR) == ('L', 0, 1)\n",
        "s2 = s1.T('DIR')\n",
        "assert (s2.p, s2.dL, s2.dR) == ('R', 0, 1)\n",
        "\n",
        "# Recompensas (λ=1; todos custos=1)\n",
        "assert abs(r(s, 'ASP', s1, lam=1, c_esq=1, c_dir=1, c_asp=1) - 0.0) < 1e-9\n",
        "assert abs(r(s1, 'DIR', s2, lam=1, c_esq=1, c_dir=1, c_asp=1) + 1.0) < 1e-9\n",
        "\n",
        "# Política reflexo\n",
        "assert politica_reflexo(('L',1)) == 'ASP'\n",
        "assert politica_reflexo(('L',0)) == 'DIR'\n",
        "assert politica_reflexo(('R',0)) == 'ESQ'\n",
        "\n",
        "# Simulação\n",
        "hist, P = simular(Env2Salas('L',1,1), lam=1, c_esq=1, c_dir=1, c_asp=1, H=20)\n",
        "assert isinstance(hist, list) and len(hist) >= 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62e0a40",
      "metadata": {
        "id": "e62e0a40"
      },
      "source": [
        "\n",
        "---\n",
        "## 4) Experimentos e Performance\n",
        "\n",
        "### Cenário A (base)\n",
        "Parâmetros: `λ=1`, `c(ESQ)=c(DIR)=c(ASP)=1`, `s0=(L,1,1)`.\n",
        "\n",
        "- Rode a simulação e mostre uma **tabela** com as colunas: `t, s, a, s_, r`.\n",
        "- Calcule o **P** final e comente o resultado (2–4 linhas).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3c2e0498",
      "metadata": {
        "id": "3c2e0498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e405761a-1c97-46c9-fb16-cb83249cc28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabela da simulação (Cenário A):\n",
            "   t                             s    a                            s_    r\n",
            "0  0  Env2Salas(p='L', dL=1, dR=1)  ASP  Env2Salas(p='L', dL=0, dR=1)  0.0\n",
            "1  1  Env2Salas(p='L', dL=0, dR=1)  DIR  Env2Salas(p='R', dL=0, dR=1) -1.0\n",
            "2  2  Env2Salas(p='R', dL=0, dR=1)  ASP  Env2Salas(p='R', dL=0, dR=0)  0.0\n",
            "\n",
            "Recompensa total (P) para o Cenário A = -1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# TODO: rode o Cenário A e exiba a tabela e o P\n",
        "s0 = Env2Salas('L', 1, 1)\n",
        "hist_A, P_A = simular(s0, lam=1, c_esq=1, c_dir=1, c_asp=1, H=20)\n",
        "df_A = pd.DataFrame(hist_A)\n",
        "\n",
        "print(\"Tabela da simulação (Cenário A):\")\n",
        "print(df_A)\n",
        "\n",
        "print(f\"\\nRecompensa total (P) para o Cenário A = {P_A}\")\n",
        "\n",
        "#   Tabela da simulação (Cenário A):\n",
        "#      t                             s    a                            s_    r\n",
        "#   0  0  Env2Salas(p='L', dL=1, dR=1)  ASP  Env2Salas(p='L', dL=0, dR=1)  0.0\n",
        "#   1  1  Env2Salas(p='L', dL=0, dR=1)  DIR  Env2Salas(p='R', dL=0, dR=1) -1.0\n",
        "#   2  2  Env2Salas(p='R', dL=0, dR=1)  ASP  Env2Salas(p='R', dL=0, dR=0)  0.0\n",
        "\n",
        "#   Recompensa total (P) para o Cenário A = -1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608a789d",
      "metadata": {
        "id": "608a789d"
      },
      "source": [
        "\n",
        "### Cenário B (limpar vale mais)\n",
        "Parâmetros: `λ=2`, custos = 1. Rode e compare com o Cenário A (2–4 linhas).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "679eac37",
      "metadata": {
        "id": "679eac37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d85791-2299-4204-d994-e21a83a4afd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P (Cenário B) = 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# TODO: Cenário B\n",
        "s0 = Env2Salas('L',1,1)\n",
        "hist_B, P_B = simular(s0, lam=2, c_esq=1, c_dir=1, c_asp=1, H=20)\n",
        "pd.DataFrame(hist_B)\n",
        "print(\"P (Cenário B) =\", P_B)\n",
        "\n",
        "# P (Cenário B) = 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a715f31e",
      "metadata": {
        "id": "a715f31e"
      },
      "source": [
        "\n",
        "### Cenário C (movimento caro)\n",
        "Parâmetros: `λ=1`, `c(ESQ)=c(DIR)=2`, `c(ASP)=1`. Discuta o impacto (2–4 linhas).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "50fc0993",
      "metadata": {
        "id": "50fc0993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0dfa193-7fd0-43c7-fefb-f2e67806331e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P (Cenário C) = -2.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# TODO: Cenário C\n",
        "s0 = Env2Salas('L',1,1)\n",
        "hist_C, P_C = simular(s0, lam=1, c_esq=2, c_dir=2, c_asp=1, H=20)\n",
        "pd.DataFrame(hist_C)\n",
        "print(\"P (Cenário C) =\", P_C)\n",
        "\n",
        "# P (Cenário C) = -2.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7433b9c7",
      "metadata": {
        "id": "7433b9c7"
      },
      "source": [
        "\n",
        "---\n",
        "## 5) Discussão e melhoria da política\n",
        "- Explique **duas limitações** do agente reflexo (sem memória).  \n",
        "  - Não consegue lidar bem com informação incompleta: se só enxerga a sujeira da sala atual, não sabe se a outra precisa de limpeza e pode acabar preso limpando sempre o mesmo lugar.\n",
        "  - Também pode agir de forma repetitiva, visto que, sem memória, repete decisões e pode ficar indo e voltando entre salas limpas, gastando energia à toa.\n",
        "- Proponha **uma melhoria simples** (ex.: 1 bit de memória) e **implemente** abaixo.\n",
        "\n",
        "  - Melhoria usando 1 bit de memória"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8daf7d01",
      "metadata": {
        "id": "8daf7d01"
      },
      "outputs": [],
      "source": [
        "def politica_melhorada(obs: Tuple[str, int], memoria: List[int]) -> str:\n",
        "    \"\"\"Política aprimorada com um bit de memória.\"\"\"\n",
        "    p, sujeira_aqui = obs\n",
        "\n",
        "    # Se a sala atual está suja, a ação é aspirar.\n",
        "    if sujeira_aqui == 1:\n",
        "        return 'ASP'\n",
        "\n",
        "    # Se a sala atual está limpa, a decisão depende da memória.\n",
        "    else:\n",
        "        # Se a outra sala ainda não foi visitada ou verificada\n",
        "        if memoria[0] == 0:\n",
        "            memoria[0] = 1 # Marca que visitou esta sala\n",
        "            return 'DIR' if p == 'L' else 'ESQ'\n",
        "        else:\n",
        "            # Ambas as salas já foram limpas\n",
        "            return 'PARAR' # Nova ação para parar o movimento\n",
        "\n",
        "def simular_melhorado(s0: Env2Salas, lam=1.0, c_esq=1.0, c_dir=1.0, c_asp=1.0, H=20):\n",
        "    historico = []\n",
        "    s = s0.copy()\n",
        "    memoria = [0] # 0 = não visitou a outra sala, 1 = visitou\n",
        "\n",
        "    for t in range(H):\n",
        "        if s.dL == 0 and s.dR == 0 and memoria[0] == 1:\n",
        "            break\n",
        "\n",
        "        obs = s.h()\n",
        "        a = politica_reflexo_com_memoria(obs, memoria)\n",
        "\n",
        "        # Implementa a nova ação 'PARAR'\n",
        "        if a == 'PARAR':\n",
        "            historico.append({'t': t, 's': s.copy(), 'a': a, 's_': s.copy(), 'r': 0.0})\n",
        "            break\n",
        "\n",
        "        s_ = s.T(a)\n",
        "        recompensa_imediata = r(s, a, s_, lam, c_esq, c_dir, c_asp)\n",
        "        historico.append({'t': t, 's': s.copy(), 'a': a, 's_': s_.copy(), 'r': recompensa_imediata})\n",
        "        s = s_\n",
        "\n",
        "    df_historico = pd.DataFrame(historico)\n",
        "    recompensa_total = df_historico['r'].sum()\n",
        "\n",
        "    return historico, recompensa_total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6543191d",
      "metadata": {
        "id": "6543191d"
      },
      "source": [
        "\n",
        "---\n",
        "## 6) Bônus (opcional)\n",
        "- **3 salas (L–C–R):** defina S, A, T, h e rode um cenário; calcule |S|.  \n",
        "- **Ruído de sujeira (prob. ρ):** estenda a transição e estime E[P] (média de 50 runs).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}